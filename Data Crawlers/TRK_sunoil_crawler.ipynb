{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file path: /Users/germankosenkov/Code projects/Crawling/3. Crawling Emails/New/TRK_imports/2024_Week_8.xlsx\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import datetime\n",
    "import pickle\n",
    "import os.path\n",
    "import base64\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import os\n",
    "import openpyxl\n",
    "import mysql.connector\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import pymysql\n",
    "import mysql.connector\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "import requests\n",
    "\n",
    "# If modifying these SCOPES, delete the file token.pickle.\n",
    "SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "\n",
    "\n",
    "#Function to handle authentication and authorization to access the Gmail API using OAuth 2.0\n",
    "def get_gmail_service():\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                '/Users/germankosenkov/Code projects/Crawling/3. Crawling Emails/New/client_secret_811893502700-tj4c2bv2942q1ua3459au95sc1ohoqle.apps.googleusercontent.com.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    service = build('gmail', 'v1', credentials=creds)\n",
    "    return service\n",
    "\n",
    "#Function to search for a message using gmail address, attachment contained in the email etc. \n",
    "def search_messages(service, query):\n",
    "    result = service.users().messages().list(userId='me', q=query).execute()\n",
    "    messages = []\n",
    "    if 'messages' in result:\n",
    "        messages.extend(result['messages'])\n",
    "    return messages\n",
    "\n",
    "\n",
    "#Function to download a specific attachment from a Gmail message and saves it to a specified directory\n",
    "def get_attachments(service, message_id, store_dir, desired_filename):\n",
    "    try:\n",
    "        message = service.users().messages().get(userId='me', id=message_id).execute()\n",
    "        parts = message.get('payload', {}).get('parts', [])\n",
    "        for part in parts:\n",
    "            if part['filename'] == desired_filename:  # Check if the filename matches the desired one\n",
    "                if 'data' in part['body']:\n",
    "                    data = part['body']['data']\n",
    "                else:\n",
    "                    att_id = part['body'].get('attachmentId')\n",
    "                    att = service.users().messages().attachments().get(userId='me', messageId=message_id, id=att_id).execute()\n",
    "                    data = att['data']\n",
    "\n",
    "                file_data = base64.urlsafe_b64decode(data.encode('UTF-8'))\n",
    "                path = os.path.join(store_dir, part['filename'])\n",
    "\n",
    "                if not os.path.exists(store_dir):\n",
    "                    os.makedirs(store_dir)\n",
    "                with open(path, 'wb') as f:\n",
    "                    f.write(file_data)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "current_date = datetime.date.today()\n",
    "current_week = current_date.isocalendar()[1]\n",
    "previous_week = current_week - 1\n",
    "current_year = datetime.date.today().year\n",
    "\n",
    "new_file_path = ''\n",
    "\n",
    "\n",
    "def main():\n",
    "    service = get_gmail_service()\n",
    "    search_query = 'from:XXXX@hotmail.com has:attachment filename:XXXX.xlsx'\n",
    "    messages = search_messages(service, search_query)\n",
    "\n",
    "    if messages:\n",
    "        latest_message = messages[0]  # Get the latest email\n",
    "\n",
    "        # Construct the path for saving the attachment with the week and year in the file name\n",
    "        #folder_name = f'{current_year}_Week_{previous_week}'\n",
    "        save_path = '/Users/germankosenkov/Code projects/Crawling/3. Crawling Emails/New/TRK_imports'\n",
    "        \n",
    "\n",
    "        # Call get_attachments with the desired filename\n",
    "        get_attachments(service, latest_message['id'], save_path, 'XXXX.xlsx')\n",
    "\n",
    "        # Path to the downloaded Excel file\n",
    "        old_file_path = os.path.join(save_path, 'XXXX.xlsx')\n",
    "        new_file_path = os.path.join(save_path, f'{current_year}_Week_{previous_week}.xlsx')\n",
    "\n",
    "        try:\n",
    "            # Rename the downloaded file\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            return new_file_path\n",
    "\n",
    "            # excel_data = pd.read_excel(new_file_path)\n",
    "            # print(excel_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading or renaming the Excel file: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    new_file_path = main()\n",
    "    print(f\"New file path: {new_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing existing data for 2023-07-31\n",
      "Replacing existing data for 2023-07-31\n",
      "Replacing existing data for 2023-07-31\n",
      "Replacing existing data for 2023-07-31\n",
      "Replacing existing data for 2023-08-31\n",
      "Replacing existing data for 2023-08-31\n",
      "Replacing existing data for 2023-08-31\n",
      "Replacing existing data for 2023-08-31\n",
      "Replacing existing data for 2023-08-31\n",
      "Replacing existing data for 2023-09-30\n",
      "Replacing existing data for 2023-09-30\n",
      "Replacing existing data for 2023-09-30\n",
      "Replacing existing data for 2023-09-30\n",
      "Replacing existing data for 2023-10-31\n",
      "Replacing existing data for 2023-10-31\n",
      "Replacing existing data for 2023-10-31\n",
      "Replacing existing data for 2023-11-30\n",
      "Replacing existing data for 2023-11-30\n",
      "Replacing existing data for 2023-11-30\n",
      "Replacing existing data for 2023-11-30\n",
      "Replacing existing data for 2023-12-31\n",
      "Replacing existing data for 2023-12-31\n",
      "Replacing existing data for 2023-12-31\n",
      "Replacing existing data for 2023-12-31\n",
      "Replacing existing data for 2023-12-31\n",
      "Replacing existing data for 2024-01-31\n",
      "Replacing existing data for 2024-01-31\n",
      "Replacing existing data for 2024-01-31\n",
      "Replacing existing data for 2024-01-31\n",
      "Replacing existing data for 2024-02-28\n",
      "Replacing existing data for 2024-02-28\n",
      "Replacing existing data for 2024-02-28\n",
      "-1 record(s) inserted.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from Constants import dates, countries\n",
    "\n",
    "#Connect to the database into which to insert the data from the crawled excel file using SSHTunnelForwarder\n",
    "b_ssh_host = 'XXXX'\n",
    "b_ssh_user = 'XXXX'\n",
    "b_ssh_port = 'XXXX'\n",
    "b_ssh_private_key = 'XXXX'\n",
    "b_sql_hostname = 'XXXX'\n",
    "b_sql_username = 'XXXX'\n",
    "b_sql_password = 'XXXX'\n",
    "b_sql_database = 'XXXX'\n",
    "b_sql_port = 'XXXX'\n",
    "\n",
    "with SSHTunnelForwarder(\n",
    "        (b_ssh_host, b_ssh_port),\n",
    "        ssh_username=b_ssh_user,\n",
    "        ssh_pkey=b_ssh_private_key,\n",
    "        remote_bind_address=(b_sql_hostname, b_sql_port)) as tunnel:\n",
    "\n",
    "    b_conn = mysql.connector.connect(\n",
    "        host='127.0.0.1',\n",
    "        user=b_sql_username,\n",
    "        passwd=b_sql_password,\n",
    "        db=b_sql_database,\n",
    "        port=tunnel.local_bind_port\n",
    "    )\n",
    "\n",
    "    b_cursor = b_conn.cursor()\n",
    "\n",
    "    # Path to the Excel file\n",
    "    workbook = openpyxl.load_workbook(new_file_path, data_only=True)\n",
    "    # Select the sheet to extract data from\n",
    "    sheet = workbook['TOPLAM']  # You can also specify the sheet name like this: sheet = workbook['Sheet1']\n",
    "\n",
    "    # Iterate through the rows and extract data from columns month,quantity,country_of_origin\n",
    "    for row in sheet.iter_rows(min_row=2, values_only=True):\n",
    "        # Assuming columns A, B, and C correspond to index 0, 1, and 2\n",
    "        month = row[5]#changed\n",
    "        quantity = row[6]\n",
    "        country_of_origin = row[7]\n",
    "\n",
    "\n",
    "        # Check if the values are not headers and not None\n",
    "        if month and quantity and country_of_origin:\n",
    "            if month != \"Ay\" and quantity != \"Miktar\" and country_of_origin != \"Ãœlke\":\n",
    "                # Find the corresponding date and country values from dictionaries\n",
    "                for date_dict in dates:\n",
    "                    for key, value in date_dict.items():\n",
    "                        if key == month:\n",
    "                            month = value\n",
    "                            break\n",
    "\n",
    "                for country_dict in countries:\n",
    "                    for key, value in country_dict.items():\n",
    "                        if key == country_of_origin:\n",
    "                            country_of_origin = value\n",
    "                            break\n",
    "\n",
    "                #Before inserting check if the data already exists in the table\n",
    "                #If exists we will update the row as in the new coming files data can be revised by the source\n",
    "                #if data doesn't exist we insert a new row \n",
    "                check_data_query = \"SELECT * FROM TradeData WHERE product_id = %s AND date = %s AND raw_trade_quantity = %s AND origin_country_id = %s AND destination_country_id_raw = %s AND data_source_id = %s\"\n",
    "                val = (859, month, quantity, country_of_origin, 180, 4)\n",
    "                b_cursor.execute(check_data_query, val)\n",
    "                existing_data = b_cursor.fetchone() \n",
    "\n",
    "                try:\n",
    "                    b_conn.autocommit = False\n",
    "\n",
    "                    if existing_data:\n",
    "                        print(\"Replacing existing data for\", month)\n",
    "                        update_query = \"UPDATE TradeData SET raw_trade_quantity = %s, destination_country_id_raw = %s, origin_country_id = %s, trade_quantity = %s, raw_unit = %s, unit = %s, frequency = %s WHERE product_id = %s AND date = %s AND data_source_id = %s AND destination_country_id_raw = %s AND origin_country_id = %s\"\n",
    "                        update_values = (quantity, 180, country_of_origin, quantity, 'mt', 'mt', 'monthly', 859, month, 4, 180, country_of_origin)\n",
    "                        b_cursor.execute(update_query, update_values)\n",
    "                    else:\n",
    "                        print(\"Inserting data for\", month)\n",
    "                        insert_query = \"INSERT INTO TradeData (product_id, date, raw_trade_quantity, destination_country_id_raw, origin_country_id, trade_quantity, raw_unit, unit, data_source_id, frequency) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "                        insert_values = (859, month, quantity, 180, country_of_origin, quantity, 'mt', 'mt', 4, 'monthly')\n",
    "                        b_cursor.execute(insert_query, insert_values)\n",
    "\n",
    "                    b_conn.commit()  # Commit the transaction\n",
    "                \n",
    "                except Exception as e:\n",
    "                    b_conn.rollback()  # Rollback the transaction if an error occurs\n",
    "                    print(\"Error occurred:\", e)\n",
    "\n",
    "                finally:\n",
    "                    b_conn.autocommit = True\n",
    "\n",
    "\n",
    "    b_cursor.close()\n",
    "    b_conn.close()\n",
    "\n",
    "print(b_cursor.rowcount, \"record(s) inserted.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a44830708761a843059adba6d554183630a5ed8b6adc3257bd6953cce1e327da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
